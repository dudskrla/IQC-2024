{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Log-in to the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import utils\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import plotly.express as px\n",
    "import helpful_functions as hf\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_SINGLE_SIGNAL = \"./1_SINGLE_SIGNAL\"\n",
    "DIR_SIGNAL_COMBINATION = \"./2_SIGNAL_COMBINATION\"\n",
    "DIR_SIGNAL_NEUTRALIZATION = \"./3_SIGNAL_NEUTRALIZATION\"\n",
    "DIR_IMPROVEMENT = \"./4_IMPROVEMENT\"\n",
    "DATA_COVERAGE = 0.75\n",
    "BUSINESS_DAY_1_YEAR = 252\n",
    "BUSINESS_DAY_6_MONTHS = 126\n",
    "\n",
    "if not os.path.exists(DIR_SINGLE_SIGNAL): os.makedirs(DIR_SINGLE_SIGNAL)\n",
    "if not os.path.exists(DIR_SIGNAL_COMBINATION): os.makedirs(DIR_SIGNAL_COMBINATION)\n",
    "if not os.path.exists(DIR_SIGNAL_NEUTRALIZATION): os.makedirs(DIR_SIGNAL_NEUTRALIZATION) \n",
    "\n",
    "s = utils.start_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset ∋ Datafield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'USA'; DELAY = 1; UNIVERSE = 'TOP3000'\n",
    "\n",
    "datasets_df = hf.get_datasets(\n",
    "    s,\n",
    "    region = REGION,\n",
    "    delay = DELAY,\n",
    "    universe = UNIVERSE\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_datasets_df = datasets_df.query(\n",
    "        \"\"\"\n",
    "        delay == 1 &\\\n",
    "        region == 'USA' &\\\n",
    "        universe == 'TOP3000' \n",
    "        \"\"\", \n",
    "        engine='python').sort_values(by=['valueScore'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>region</th>\n",
       "      <th>delay</th>\n",
       "      <th>universe</th>\n",
       "      <th>coverage</th>\n",
       "      <th>turnover</th>\n",
       "      <th>valueScore</th>\n",
       "      <th>userCount</th>\n",
       "      <th>alphaCount</th>\n",
       "      <th>fieldCount</th>\n",
       "      <th>researchPapers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>univ1</td>\n",
       "      <td>Universe Dataset</td>\n",
       "      <td>No dataset description</td>\n",
       "      <td>pv</td>\n",
       "      <td>pv-price-volume</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news18</td>\n",
       "      <td>Ravenpack News Data</td>\n",
       "      <td>This dataset provides news sentiment and other...</td>\n",
       "      <td>news</td>\n",
       "      <td>news-news-sentiment</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.6779</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>586</td>\n",
       "      <td>7880</td>\n",
       "      <td>75</td>\n",
       "      <td>[{'title': 'Research Paper 01: The Momentum of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analyst4</td>\n",
       "      <td>Analyst Estimate Data for Equity</td>\n",
       "      <td>This dataset provides details and aggregations...</td>\n",
       "      <td>analyst</td>\n",
       "      <td>analyst-analyst-estimates</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3527</td>\n",
       "      <td>51966</td>\n",
       "      <td>350</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fundamental2</td>\n",
       "      <td>Report Footnotes</td>\n",
       "      <td>This dataset holds fundamental items included ...</td>\n",
       "      <td>fundamental</td>\n",
       "      <td>fundamental-footnotes</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3427</td>\n",
       "      <td>30340</td>\n",
       "      <td>318</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fundamental6</td>\n",
       "      <td>Company Fundamental Data for Equity</td>\n",
       "      <td>Fundamental database covers most of the world'...</td>\n",
       "      <td>fundamental</td>\n",
       "      <td>fundamental-fundamental-data</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.7357</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22727</td>\n",
       "      <td>669992</td>\n",
       "      <td>914</td>\n",
       "      <td>[{'title': 'Research Paper 03: Cross-Firm Info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model16</td>\n",
       "      <td>Fundamental Scores</td>\n",
       "      <td>This dataset ranks stocks based on fundamental...</td>\n",
       "      <td>model</td>\n",
       "      <td>model-valuation-models</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>635</td>\n",
       "      <td>8</td>\n",
       "      <td>[{'title': 'Research Paper 01: The Momentum of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model38</td>\n",
       "      <td>Growth Valuation Model</td>\n",
       "      <td>This dataset is a stock ranking model that sor...</td>\n",
       "      <td>model</td>\n",
       "      <td>model-valuation-models</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.7192</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>451</td>\n",
       "      <td>22408</td>\n",
       "      <td>57</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model51</td>\n",
       "      <td>Systematic Risk Metrics</td>\n",
       "      <td>This is a risk-model data offering several met...</td>\n",
       "      <td>model</td>\n",
       "      <td>model-risk-models</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1602</td>\n",
       "      <td>5089</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news12</td>\n",
       "      <td>US News Data</td>\n",
       "      <td>This dataset specializes in matching financial...</td>\n",
       "      <td>news</td>\n",
       "      <td>news-news</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.8034</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5068</td>\n",
       "      <td>40993</td>\n",
       "      <td>322</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>option8</td>\n",
       "      <td>Volatility Data</td>\n",
       "      <td>This is an option dataset which provides histo...</td>\n",
       "      <td>option</td>\n",
       "      <td>option-option-volatility</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2127</td>\n",
       "      <td>27982</td>\n",
       "      <td>64</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>option9</td>\n",
       "      <td>Options Analytics</td>\n",
       "      <td>This dataset provide options metrics indicatin...</td>\n",
       "      <td>option</td>\n",
       "      <td>option-option-analytics</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>9659</td>\n",
       "      <td>74</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pv1</td>\n",
       "      <td>Price Volume Data for Equity</td>\n",
       "      <td>A dataset containing price, volume, close, ope...</td>\n",
       "      <td>pv</td>\n",
       "      <td>pv-price-volume</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25915</td>\n",
       "      <td>811232</td>\n",
       "      <td>18</td>\n",
       "      <td>[{'title': 'Research Paper 03: Cross-Firm Info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pv13</td>\n",
       "      <td>Relationship Data for Equity</td>\n",
       "      <td>The dataset outputs various classifications an...</td>\n",
       "      <td>pv</td>\n",
       "      <td>pv-relationship</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.8109</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2470</td>\n",
       "      <td>21545</td>\n",
       "      <td>168</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>socialmedia12</td>\n",
       "      <td>Sentiment Data for Equity</td>\n",
       "      <td>This dataset provides sentiment data with diff...</td>\n",
       "      <td>socialmedia</td>\n",
       "      <td>socialmedia-social-media</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.9645</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3375</td>\n",
       "      <td>8904</td>\n",
       "      <td>12</td>\n",
       "      <td>[{'title': 'Research Paper 01: The Momentum of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>socialmedia8</td>\n",
       "      <td>Social Media Data for Equity</td>\n",
       "      <td>This is a sentiment dataset based on tweets or...</td>\n",
       "      <td>socialmedia</td>\n",
       "      <td>socialmedia-social-media</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>TOP3000</td>\n",
       "      <td>0.8585</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1224</td>\n",
       "      <td>3772</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                 name  \\\n",
       "14          univ1                     Universe Dataset   \n",
       "7          news18                  Ravenpack News Data   \n",
       "0        analyst4     Analyst Estimate Data for Equity   \n",
       "1    fundamental2                     Report Footnotes   \n",
       "2    fundamental6  Company Fundamental Data for Equity   \n",
       "3         model16                   Fundamental Scores   \n",
       "4         model38               Growth Valuation Model   \n",
       "5         model51              Systematic Risk Metrics   \n",
       "6          news12                         US News Data   \n",
       "8         option8                      Volatility Data   \n",
       "9         option9                    Options Analytics   \n",
       "10            pv1         Price Volume Data for Equity   \n",
       "11           pv13         Relationship Data for Equity   \n",
       "12  socialmedia12            Sentiment Data for Equity   \n",
       "13   socialmedia8         Social Media Data for Equity   \n",
       "\n",
       "                                          description     category  \\\n",
       "14                             No dataset description           pv   \n",
       "7   This dataset provides news sentiment and other...         news   \n",
       "0   This dataset provides details and aggregations...      analyst   \n",
       "1   This dataset holds fundamental items included ...  fundamental   \n",
       "2   Fundamental database covers most of the world'...  fundamental   \n",
       "3   This dataset ranks stocks based on fundamental...        model   \n",
       "4   This dataset is a stock ranking model that sor...        model   \n",
       "5   This is a risk-model data offering several met...        model   \n",
       "6   This dataset specializes in matching financial...         news   \n",
       "8   This is an option dataset which provides histo...       option   \n",
       "9   This dataset provide options metrics indicatin...       option   \n",
       "10  A dataset containing price, volume, close, ope...           pv   \n",
       "11  The dataset outputs various classifications an...           pv   \n",
       "12  This dataset provides sentiment data with diff...  socialmedia   \n",
       "13  This is a sentiment dataset based on tweets or...  socialmedia   \n",
       "\n",
       "                     subcategory region  delay universe  coverage turnover  \\\n",
       "14               pv-price-volume    USA      1  TOP3000    0.4480     None   \n",
       "7            news-news-sentiment    USA      1  TOP3000    0.6779     None   \n",
       "0      analyst-analyst-estimates    USA      1  TOP3000    0.6036     None   \n",
       "1          fundamental-footnotes    USA      1  TOP3000    0.4089     None   \n",
       "2   fundamental-fundamental-data    USA      1  TOP3000    0.7357     None   \n",
       "3         model-valuation-models    USA      1  TOP3000    0.3077     None   \n",
       "4         model-valuation-models    USA      1  TOP3000    0.7192     None   \n",
       "5              model-risk-models    USA      1  TOP3000    0.7724     None   \n",
       "6                      news-news    USA      1  TOP3000    0.8034     None   \n",
       "8       option-option-volatility    USA      1  TOP3000    0.6948     None   \n",
       "9        option-option-analytics    USA      1  TOP3000    0.7039     None   \n",
       "10               pv-price-volume    USA      1  TOP3000    1.0000     None   \n",
       "11               pv-relationship    USA      1  TOP3000    0.8109     None   \n",
       "12      socialmedia-social-media    USA      1  TOP3000    0.9645     None   \n",
       "13      socialmedia-social-media    USA      1  TOP3000    0.8585     None   \n",
       "\n",
       "    valueScore  userCount  alphaCount  fieldCount  \\\n",
       "14         3.0         13          25           5   \n",
       "7          2.0        586        7880          75   \n",
       "0          1.0       3527       51966         350   \n",
       "1          1.0       3427       30340         318   \n",
       "2          1.0      22727      669992         914   \n",
       "3          1.0        133         635           8   \n",
       "4          1.0        451       22408          57   \n",
       "5          1.0       1602        5089          16   \n",
       "6          1.0       5068       40993         322   \n",
       "8          1.0       2127       27982          64   \n",
       "9          1.0       2004        9659          74   \n",
       "10         1.0      25915      811232          18   \n",
       "11         1.0       2470       21545         168   \n",
       "12         1.0       3375        8904          12   \n",
       "13         1.0       1224        3772           2   \n",
       "\n",
       "                                       researchPapers  \n",
       "14                                                 []  \n",
       "7   [{'title': 'Research Paper 01: The Momentum of...  \n",
       "0                                                  []  \n",
       "1                                                  []  \n",
       "2   [{'title': 'Research Paper 03: Cross-Firm Info...  \n",
       "3   [{'title': 'Research Paper 01: The Momentum of...  \n",
       "4                                                  []  \n",
       "5                                                  []  \n",
       "6                                                  []  \n",
       "8                                                  []  \n",
       "9                                                  []  \n",
       "10  [{'title': 'Research Paper 03: Cross-Firm Info...  \n",
       "11                                                 []  \n",
       "12  [{'title': 'Research Paper 01: The Momentum of...  \n",
       "13                                                 []  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_datasets_df.sort_values('valueScore',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Signal Browsing in Each Datafield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "univ1 0\n",
      "news18 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":\"You do not have permission to perform this action.\"}\n",
      "{\"detail\":\"You do not have permission to perform this action.\"}\n",
      "{\"detail\":\"You do not have permission to perform this action.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.07s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":\"You do not have permission to perform this action.\"}\n",
      "{\"detail\":\"You do not have permission to perform this action.\"}\n",
      "{\"detail\":\"You do not have permission to perform this action.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:11<00:00, 17.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_result: 1 | print: {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_acb),252) )'}\n",
      "[[{'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_acb),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_bam),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_bee),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_ber),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_event_relevance),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_event_similarity_days),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_ghc_lna),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_nip),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': None, 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_qcm),252) )'}, 'is_stats': None, 'pnl': None, 'stats': None, 'is_tests': None}, {'alpha_id': 'vmALPJd', 'simulate_data': {'type': 'REGULAR', 'settings': {'nanHandling': 'OFF', 'instrumentType': 'EQUITY', 'delay': 1, 'universe': 'TOP3000', 'truncation': 0.08, 'unitHandling': 'VERIFY', 'pasteurization': 'ON', 'region': 'USA', 'language': 'FASTEXPR', 'decay': 0, 'neutralization': 'INDUSTRY', 'visualization': False}, 'regular': 'rank( ts_backfill(vec_avg(nws18_qep),252) )'}, 'is_stats':       pnl  bookSize  longCount  shortCount  turnover  returns  drawdown  \\\n",
      "0  547372  20000000       1512        1552    0.9409    0.011    0.0873   \n",
      "\n",
      "     margin  fitness  sharpe   startDate alpha_id  \n",
      "0  0.000023     0.03    0.31  2017-01-22  vmALPJd  , 'pnl': None, 'stats': None, 'is_tests':                       name   result  limit   value  \\\n",
      "0               LOW_SHARPE     FAIL   1.25  0.3100   \n",
      "1              LOW_FITNESS     FAIL   1.00  0.0300   \n",
      "2             LOW_TURNOVER     PASS   0.01  0.9409   \n",
      "3            HIGH_TURNOVER     FAIL   0.70  0.9409   \n",
      "4      CONCENTRATED_WEIGHT     PASS    NaN     NaN   \n",
      "5  LOW_SUB_UNIVERSE_SHARPE     FAIL   0.13 -0.1200   \n",
      "6         SELF_CORRELATION  PENDING    NaN     NaN   \n",
      "7      MATCHES_COMPETITION     PASS    NaN     NaN   \n",
      "\n",
      "                                             matched unmatched alpha_id  \n",
      "0                                                NaN       NaN  vmALPJd  \n",
      "1                                                NaN       NaN  vmALPJd  \n",
      "2                                                NaN       NaN  vmALPJd  \n",
      "3                                                NaN       NaN  vmALPJd  \n",
      "4                                                NaN       NaN  vmALPJd  \n",
      "5                                                NaN       NaN  vmALPJd  \n",
      "6                                                NaN       NaN  vmALPJd  \n",
      "7  [{'id': 'challenge', 'name': 'Challenge'}, {'i...        []  vmALPJd  }]]\n",
      "err:news18\n",
      "analyst4 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":\"You do not have permission to perform this action.\"}\n",
      "{\"detail\":\"You do not have permission to perform this action.\"}\n",
      "{\"detail\":\"You do not have permission to perform this action.\"}\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in selected_datasets_df.sort_values('valueScore',ascending=False).id.values:    \n",
    "\n",
    "    # 1) LOAD DATAFIELDS\n",
    "    datafields_df = hf.get_datafields(s, dataset_id=dataset_id).query(f\"{DATA_COVERAGE} < coverage\")\n",
    "    \n",
    "    # 2) MAKE EXPRESSION LIST\n",
    "    # ts_backfill : to backfill empty data\n",
    "    # 5 operators : rank, scale, ts_skewness, ts_rank, ts_zscore\n",
    "    expression_list = \\\n",
    "    [f'rank( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}) )' for datafield in datafields_df.iloc]+\\\n",
    "    [f'scale( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}) )' for datafield in datafields_df.iloc]+\\\n",
    "    [f'ts_skewness( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}),{BUSINESS_DAY_6_MONTHS} )' for datafield in datafields_df.iloc]+\\\n",
    "    [f'ts_rank( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}),{BUSINESS_DAY_6_MONTHS} )' for datafield in datafields_df.iloc]+\\\n",
    "    [f'ts_zscore( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}),{BUSINESS_DAY_6_MONTHS} )' for datafield in datafields_df.iloc]\n",
    "\n",
    "    if len(os.listdir(DIR_SINGLE_SIGNAL+\"/\"))>0:\n",
    "        existed = list(pd.concat([pd.read_csv(DIR_SINGLE_SIGNAL+\"/\"+path).drop('Unnamed: 0',axis=1) for path in os.listdir(DIR_SINGLE_SIGNAL) ]).reset_index(drop=True).regular)\n",
    "        expression_list = [x for x in expression_list if x not in existed] # if there are saved data already simulated, then drop duplicated expressions\n",
    "\n",
    "    # 3) GENERATE ALPHA LIST\n",
    "    alpha_list = [utils.generate_alpha(x, region=REGION, universe=UNIVERSE,) for x in expression_list]\n",
    "    print(dataset_id, len(expression_list))\n",
    "\n",
    "    # 4) MULSISIMULATE ALPHAS\n",
    "    result = []\n",
    "    try:\n",
    "        for x in range(0,len(alpha_list),10):\n",
    "            \n",
    "            temp_result = [] \n",
    "            result.append(utils.simulate_alpha_list_multi(s, alpha_list[x:x+10 if x+10 < len(alpha_list) else len(alpha_list)]))\n",
    "            temp_result.append(utils.simulate_alpha_list_multi(s, alpha_list[x:x+10 if x+10 < len(alpha_list) else len(alpha_list)]))\n",
    "            print(f\"temp_result: {len(temp_result)} | print: {alpha_list[x]}\")\n",
    "            print(temp_result)\n",
    "            temp_df = pd.concat([pd.DataFrame(pd.concat([pd.Series(data = [res['simulate_data']['regular']],index = ['regular']),pd.Series(res['simulate_data']['settings']),res['is_stats'].iloc[0],pd.Series(data =  res['is_tests'].result.values , index = res['is_tests'].name)])).T for res in  result[-1]]).reset_index(drop=True)\n",
    "            print(\"temp_df\")\n",
    "            print(temp_df)\n",
    "            \n",
    "            if x == 0:\n",
    "                if os.path.exists(DIR_SINGLE_SIGNAL+f\"/{dataset_id}_layer1.csv\"):\n",
    "                    res_df = pd.read_csv(DIR_SINGLE_SIGNAL+f'/{dataset_id}_layer1.csv').drop('Unnamed: 0',axis=1)\n",
    "                    res_df = pd.concat([res_df,temp_df])\n",
    "                else:\n",
    "                    res_df = temp_df\n",
    "            else:\n",
    "                res_df = pd.concat([res_df,temp_df])\n",
    "            res_df.to_csv(DIR_SINGLE_SIGNAL+f'/{dataset_id}_layer1.csv')\n",
    "    except:\n",
    "        print(f'err:{dataset_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Signal Combining in Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_signal(dataset_id): # pick signal according to its sharpe ratio\n",
    "    df = pd.read_csv(DIR_SINGLE_SIGNAL+f\"/{dataset_id}_layer1.csv\")\n",
    "    df_cand = df[(df['CONCENTRATED_WEIGHT'] ==\"PASS\")&(df['sharpe']!=0)].copy()\n",
    "    sharpe = df_cand.sharpe\n",
    "    prob = (sharpe**4).clip(upper=8)\n",
    "    prob_cum = (prob/prob.sum()).cumsum()\n",
    "    df_cand['prob_cum'] = prob_cum\n",
    "    randnum = rand.random()\n",
    "    return {\n",
    "        'regular':df_cand[df_cand.prob_cum>randnum].iloc[0].regular, \n",
    "        'sharpe':df_cand[df_cand.prob_cum>randnum].iloc[0].sharpe\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id in selected_datasets_df.sort_values('valueScore', ascending=False).id.values:\n",
    "    try:\n",
    "        # 1) LOAD DATAFIELDS\n",
    "        datafields_df = hf.get_datafields(s, dataset_id=dataset_id).query(f\"{DATA_COVERAGE} < coverage\")\n",
    "        \n",
    "        # 2) MAKE EXPRESSION LIST\n",
    "        expression_list = \\\n",
    "            [f'rank( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}) )' for datafield in datafields_df.iloc]+\\\n",
    "            [f'scale( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}) )' for datafield in datafields_df.iloc]+\\\n",
    "            [f'ts_skewness( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}),{BUSINESS_DAY_6_MONTHS} )' for datafield in datafields_df.iloc]+\\\n",
    "            [f'ts_rank( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}),{BUSINESS_DAY_6_MONTHS} )' for datafield in datafields_df.iloc]+\\\n",
    "            [f'ts_zscore( ts_backfill({ \"vec_avg\" if datafield.type == \"VECTOR\" else \"\" }({datafield.id}),{BUSINESS_DAY_1_YEAR}),{BUSINESS_DAY_6_MONTHS} )' for datafield in datafields_df.iloc]\n",
    "\n",
    "        if len(os.listdir(DIR_SINGLE_SIGNAL+\"/\")) > 0:\n",
    "            existed = list(pd.concat([pd.read_csv(DIR_SINGLE_SIGNAL+\"/\" + path).drop('Unnamed: 0', axis=1) for path in os.listdir(DIR_SINGLE_SIGNAL)]).reset_index(drop=True).regular)\n",
    "            expression_list = [x for x in expression_list if x not in existed] # if there are saved data already simulated, then drop duplicated expressions\n",
    "\n",
    "        # 3) GENERATE ALPHA LIST\n",
    "        alpha_list = [utils.generate_alpha(x, region=\"USA\", universe=\"TOP3000\") for x in expression_list]\n",
    "        print(dataset_id, len(expression_list))\n",
    "\n",
    "        # 4) MULTISIMULATE ALPHS\n",
    "        result = []\n",
    "        with tqdm(total=len(alpha_list), desc=f'Simulating {dataset_id}') as pbar:\n",
    "            for x in range(0, len(alpha_list), 10):\n",
    "                try:\n",
    "                    current_alphas = alpha_list[x:x + 10 if x + 10 < len(alpha_list) else len(alpha_list)]\n",
    "                    print(f\"Simulating alphas from {x} to {x + len(current_alphas) - 1}\")\n",
    "                    result.append(utils.simulate_alpha_list_multi(s, current_alphas))\n",
    "\n",
    "                    temp_df = pd.concat([\n",
    "                        pd.DataFrame(pd.concat([pd.Series(data=[res['simulate_data']['regular']], index=['regular']), pd.Series(res['simulate_data']['settings']), res['is_stats'].iloc[0], pd.Series(data=res['is_tests'].result.values, index=res['is_tests'].name)])).T\n",
    "                        for res in result[-1]\n",
    "                    ]).reset_index(drop=True)\n",
    "\n",
    "                    print(f\"result: {len(result)} | print: {current_alphas}\")\n",
    "                    print(result)\n",
    "                    \n",
    "                    if x == 0:\n",
    "                        if os.path.exists(DIR_SINGLE_SIGNAL+f\"/{dataset_id}_layer1.csv\"):\n",
    "                            res_df = pd.read_csv(DIR_SINGLE_SIGNAL+f'/{dataset_id}_layer1.csv').drop('Unnamed: 0', axis=1)\n",
    "                            res_df = pd.concat([res_df, temp_df])\n",
    "                        else:\n",
    "                            res_df = temp_df\n",
    "                    else:\n",
    "                        res_df = pd.concat([res_df, temp_df])\n",
    "\n",
    "                    res_df.to_csv(DIR_SINGLE_SIGNAL+f'/{dataset_id}_layer1.csv')\n",
    "                except Exception as e:\n",
    "                    print(f'Error processing alpha list segment {x} for dataset {dataset_id}: {str(e)}')\n",
    "                finally:\n",
    "                    pbar.update(len(current_alphas))\n",
    "    except Exception as e:\n",
    "        print(f'Error processing dataset {dataset_id}: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id in  [x[:x.index('_')] for x in os.listdir(DIR_SINGLE_SIGNAL+\"/\")]:\n",
    "\n",
    "    # if layer1 simulation for dataset is not done, pass the loop\n",
    "    if f\"{dataset_id}_layer1.csv\" not in os.listdir(DIR_SINGLE_SIGNAL+\"/\"):\n",
    "        continue\n",
    "\n",
    "    # if there is no meaningful signals in layer1 simulations, pass the loop\n",
    "    layer1_alphas = pd.read_csv(DIR_SINGLE_SIGNAL+f\"/{dataset_id}_layer1.csv\")\n",
    "    if len(layer1_alphas[abs(layer1_alphas.sharpe)>=1]) == 0:\n",
    "        continue\n",
    "    \n",
    "    # 1) MAKE 500 COMVINATION EXPRESSIONS\n",
    "    expression_list = []\n",
    "    for i in range(500):\n",
    "        signal1 = pick_signal(dataset_id)\n",
    "        signal2 = pick_signal(dataset_id)\n",
    "        signal_def = f\"\"\"signal1 = scale({signal1['regular']});\n",
    "                        signal2 = scale({signal2['regular']});\n",
    "                        \"\"\"\n",
    "        '' if signal1['sharpe']*signal2['sharpe']>0 else '-'\n",
    "\n",
    "        comb_cand = [f\"{'' if signal1['sharpe']*signal2['sharpe']>0 else '-'}signal1*(1+signal2)\",\n",
    "        f\"{'' if signal1['sharpe']>0 else '-'}vector_neut(signal1,signal2)\",\n",
    "        f\"{'' if signal1['sharpe']>0 else '-'}vector_proj(signal1,signal2)\",\n",
    "        f\"{'' if signal1['sharpe']>0 else '-'}regression_neut(signal1,signal2)\",\n",
    "        f\"{'' if signal1['sharpe']>0 else '-'}regression_proj(signal1,signal2)\"]\n",
    "        expression_list.append(signal_def+comb_cand[rand.randint(0,4)])\n",
    "    expression_list = list(dict.fromkeys(expression_list))\n",
    "    \n",
    "    if len(os.listdir(DIR_SIGNAL_COMBINATION))>0:\n",
    "        existed = list(pd.concat([pd.read_csv(DIR_SIGNAL_COMBINATION+\"/\"+path).drop('Unnamed: 0',axis=1) for path in os.listdir(DIR_SIGNAL_COMBINATION) ]).reset_index(drop=True).regular)\n",
    "        expression_list = [x for x in expression_list if x not in existed] # if there are saved data already simulated, then drop duplicated expressions\n",
    "    \n",
    "    # 2) GENERATE ALPHA LIST\n",
    "    alpha_list = [utils.generate_alpha(x, region=REGION, universe=UNIVERSE,) for x in expression_list]\n",
    "    print(dataset_id, len(expression_list))\n",
    "\n",
    "    # 3) MULTISIMULATE ALPHAS\n",
    "    result = []\n",
    "    try:\n",
    "        for x in range(0,len(alpha_list),10):\n",
    "        \n",
    "            result.append(utils.simulate_alpha_list_multi(s, alpha_list[x:x+10 if x+10 < len(alpha_list) else len(alpha_list)]))\n",
    "\n",
    "            temp_df = pd.concat([pd.DataFrame(pd.concat([pd.Series(data = [res['simulate_data']['regular']],index = ['regular']),pd.Series(res['simulate_data']['settings']),res['is_stats'].iloc[0],pd.Series(data =  res['is_tests'].result.values , index = res['is_tests'].name)])).T for res in  result[-1]]).reset_index(drop=True)\n",
    "            \n",
    "            if x == 0:\n",
    "                if os.path.exists(DIR_SIGNAL_COMBINATION+f\"/{dataset_id}_layer2.csv\"):\n",
    "                    res_df = pd.read_csv(DIR_SIGNAL_COMBINATION+f'/{dataset_id}_layer2.csv').drop('Unnamed: 0',axis=1)\n",
    "                    res_df = pd.concat([res_df,temp_df])\n",
    "                else:\n",
    "                    res_df = temp_df\n",
    "            else:\n",
    "                res_df = pd.concat([res_df,temp_df])\n",
    "            \n",
    "            res_df.to_csv(DIR_SIGNAL_COMBINATION+f'/{dataset_id}_layer2.csv')\n",
    "    except:\n",
    "        print(f'err:{dataset_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neutralize Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick combined signal according to its sharpe ratio\n",
    "def pick_signal_3():\n",
    "    df = pd.read_csv(DIR_SIGNAL_COMBINATION+\"/{dataset_id}_layer2.csv\")\n",
    "    df_cand = df[(df['CONCENTRATED_WEIGHT'] ==\"PASS\")&(df['sharpe']!=0)].copy()\n",
    "    sharpe = df_cand.sharpe\n",
    "    prob = (sharpe**4).clip(upper=8)\n",
    "    prob_cum = (prob/prob.sum()).cumsum()\n",
    "    df_cand['prob_cum'] = prob_cum\n",
    "    randnum = rand.random()\n",
    "    \n",
    "    return {'regular':df_cand[df_cand.prob_cum>randnum].iloc[0].regular, 'sharpe':df_cand[df_cand.prob_cum>randnum].iloc[0].sharpe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) LOAD GROUP DATAFIELDS\n",
    "groups = list(pd.concat([hf.get_datafields(s, dataset_id=dataset_id).query(f\"{DATA_COVERAGE} < coverage\") for dataset_id in ['pv13','pv29','pv30']]).id)\n",
    "\n",
    "for dataset_id in [x[:x.index('_')] for x in os.listdir('./layer2_2')]:\n",
    "\n",
    "    # if layer2 simulation for dataset is not done, pass the loop\n",
    "    if not os.path.exists(DIR_SINGLE_SIGNAL):\n",
    "        continue\n",
    "    \n",
    "    # if there is no meaningful signals in layer2 simulations, pass the loop\n",
    "    layer2_alphas = pd.read_csv(DIR_SIGNAL_COMBINATION+\"/{dataset_id}_layer2.csv\")\n",
    "    if len(layer2_alphas[abs(layer2_alphas.sharpe)>=1]) == 0:\n",
    "        continue\n",
    "\n",
    "    # 2) MAKE 500 COMBINATION EXPRESSIONS\n",
    "    expression_list = []\n",
    "    for i in range(500):\n",
    "        signal = pick_signal_3(dataset_id)\n",
    "        signal_regular = signal['regular']\n",
    "        signal_last_line = [f'alpha = '+signal_regular.split('\\n')[-1]+';']\n",
    "        randnum = rand.randint(0,len(groups))\n",
    "        last_line = [f'{\"\" if signal[\"sharpe\"]>0 else \"-\"}group_neutralize(alpha, densify({groups[randnum]}))' if randnum != len(groups) else 'alpha']\n",
    "        expression_list.append('\\n'.join(signal_regular.split('\\n')[:-1] + signal_last_line + last_line))\n",
    "    expression_list = list(dict.fromkeys(expression_list))\n",
    "    \n",
    "    if len(os.listdir(DIR_SIGNAL_NEUTRALIZATION))>0:\n",
    "        existed = list(pd.concat([pd.read_csv(DIR_SIGNAL_NEUTRALIZATION+\"/\"+path).drop('Unnamed: 0',axis=1) for path in os.listdir(DIR_SIGNAL_NEUTRALIZATION) ]).reset_index(drop=True).regular)\n",
    "        expression_list = [x for x in expression_list if x not in existed] # if there are saved data already simulated, then drop duplicated expressions\n",
    "    \n",
    "    # 3) GENERATE ALPHA LIST\n",
    "    alpha_list = [utils.generate_alpha(x, region=REGION, universe=UNIVERSE,neutralization=[\"MARKET\",\"INDUSTRY\",\"SLOW\",\"FAST\",\"SLOW_AND_FAST\"][rand.randint(0,4)]) for x in expression_list]\n",
    "    print(dataset_id, len(expression_list))\n",
    "\n",
    "    # 4) MULTISIMULATE ALPHAS\n",
    "    result = []\n",
    "    try:\n",
    "        for x in range(0,len(alpha_list),10):\n",
    "        \n",
    "            result.append(utils.simulate_alpha_list_multi(s, alpha_list[x:x+10 if x+10 < len(alpha_list) else len(alpha_list)]))\n",
    "\n",
    "            temp_df = pd.concat([pd.DataFrame(pd.concat([pd.Series(data = [res['simulate_data']['regular']],index = ['regular']),pd.Series(res['simulate_data']['settings']),res['is_stats'].iloc[0],pd.Series(data =  res['is_tests'].result.values , index = res['is_tests'].name)])).T for res in  result[-1]]).reset_index(drop=True)\n",
    "            \n",
    "            if x == 0:\n",
    "                if os.path.exists(DIR_SIGNAL_NEUTRALIZATION+f\"/{dataset_id}_layer3.csv\"):\n",
    "                    res_df = pd.read_csv(DIR_SIGNAL_NEUTRALIZATION+f'/{dataset_id}_layer3.csv').drop('Unnamed: 0',axis=1)\n",
    "                    res_df = pd.concat([res_df,temp_df])\n",
    "                else:\n",
    "                    res_df = temp_df\n",
    "            else:\n",
    "                res_df = pd.concat([res_df,temp_df])\n",
    "            \n",
    "            res_df.to_csv(DIR_SIGNAL_NEUTRALIZATION+f'/{dataset_id}_layer3.csv')\n",
    "    except:\n",
    "        print(f'err:{dataset_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improvement\n",
    "Find the local optimum for alphas with good performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) decrease turnover: this process improves alphas with high turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from all layers result, find candidate alphas \n",
    "# (that pass the IS_LADDER_SHARPE, LOW_SHARPE, and CONCENTRATED_WEIGHT tests)\n",
    "df = pd.concat(sum([[pd.read_csv(f'./{layer_name}_2/'+x).drop('Unnamed: 0',axis =1) for x in os.listdir(f'./{layer_name}_2/')] for layer_name in ['layer1','layer2','layer3']],[])).reset_index(drop=True)\n",
    "\n",
    "candidate = df.query(\"\"\"\n",
    "IS_LADDER_SHARPE == 'PASS' &\\\n",
    "LOW_SHARPE == 'PASS' &\\\n",
    "CONCENTRATED_WEIGHT == 'PASS' \n",
    "    \"\"\", engine='python')\n",
    "\n",
    "# 1) MAKE TURNOVER DECREASING GRID\n",
    "imp_alpha_list = sum([utils.alpha_grid_decrease_turnover(s,alpha_id) for alpha_id in list(candidate.alpha_id)],[])\n",
    "\n",
    "if os.path.exists(DIR_IMPROVEMENT+f'/improved_decrease_turnover.csv'):\n",
    "    stored = pd.read_csv(DIR_IMPROVEMENT+f'/improved_decrease_turnover.csv').drop('Unnamed: 0',axis =1)\n",
    "    for alpha in imp_alpha_list: \n",
    "        if len(stored[(stored.regular == alpha[0]) & (stored.neutralization == alpha[1]) & (stored.decay ==alpha[2])])>0:\n",
    "            imp_alpha_list.remove(alpha) # drop duplicates\n",
    "\n",
    "# 2) MAKE ALPHA LIST\n",
    "alpha_list = [utils.generate_alpha(alpha[0], region=REGION, universe=UNIVERSE,neutralization=alpha[1],decay = alpha[2]) for alpha in imp_alpha_list]\n",
    "\n",
    "# 3) MULTISIMULATE ALPHAS\n",
    "iteration = 0\n",
    "result = []\n",
    "for x in range(0,len(alpha_list),10):\n",
    "    iteration += 1\n",
    "    try:\n",
    "        result.append(utils.simulate_alpha_list_multi(s, alpha_list[x:x+10 if x+10 < len(alpha_list) else len(alpha_list)]))\n",
    "        temp_df = pd.concat([\\\n",
    "            pd.concat([pd.DataFrame(pd.concat([pd.Series(data = [res['simulate_data']['regular']],index = ['regular']),pd.Series(res['simulate_data']['settings']),res['is_stats'].iloc[0],pd.Series(data =  res['is_tests'].result.values , index = res['is_tests'].name)])).T for res in  result[-1]]).reset_index(drop=True),\\\n",
    "            pd.DataFrame(data = zip(*[[imp_alpha_list[x][3] for x in range(x,x+10)],[imp_alpha_list[x][4] for x in range(x,x+10)]]),columns = ['mother_alpha','grandmother_alpha']).reset_index(drop=True)]\\\n",
    "            ,axis=1)\n",
    "            \n",
    "        if x == 0:\n",
    "            if os.path.exists(DIR_IMPROVEMENT+f\"/improved_decrease_turnover.csv\"):\n",
    "                res_df = pd.read_csv(DIR_IMPROVEMENT+f'/improved_decrease_turnover.csv').drop('Unnamed: 0',axis=1)\n",
    "                res_df = pd.concat([res_df,temp_df])\n",
    "            else:\n",
    "                res_df = temp_df\n",
    "        else:\n",
    "            res_df = pd.concat([res_df,temp_df])\n",
    "            \n",
    "        res_df.reset_index(drop=True).to_csv(DIR_IMPROVEMENT+f'/improved_decrease_turnover.csv')\n",
    "    except:\n",
    "        print(f'err, iteration = {iteration}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) change time horizon: this process tests alphas using ts_oprators with different time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from all layers result, find candidate alphas \n",
    "# (that pass the IS_LADDER_SHARPE, LOW_SHARPE, and CONCENTRATED_WEIGHT tests)\n",
    "df = pd.concat(sum([[pd.read_csv(f'./{layer_name}_2/'+x).drop('Unnamed: 0',axis =1) for x in os.listdir(f'./{layer_name}_2/')] for layer_name in ['layer1','layer2','layer3']],[])).reset_index(drop=True)\n",
    "\n",
    "candidate = df.query(\"\"\"\n",
    "IS_LADDER_SHARPE == 'PASS' &\\\n",
    "LOW_SHARPE == 'PASS' &\\\n",
    "CONCENTRATED_WEIGHT == 'PASS' \n",
    "    \"\"\", engine='python')\n",
    "\n",
    "# 1) MAKE TIME HORIZON GRID\n",
    "imp_alpha_list = sum([utils.alpha_grid_time_horizon(s,alpha_id) for alpha_id in list(candidate.alpha_id)],[])\n",
    "imp_alpha_list_original = imp_alpha_list.copy()\n",
    "print(len(imp_alpha_list_original))\n",
    "\n",
    "if os.path.exists(DIR_IMPROVEMENT+f'/improved_time_horizon.csv'):\n",
    "    stored = pd.read_csv(DIR_IMPROVEMENT+f'/improved_time_horizon.csv').drop('Unnamed: 0',axis =1)\n",
    "    for alpha in imp_alpha_list_original:  ## drop duplicates\n",
    "        if len(stored[(stored.regular == alpha[0]) & (stored.neutralization == alpha[1]) & (stored.decay ==alpha[2])])>0:\n",
    "            imp_alpha_list.remove(alpha) # drop duplicates\n",
    "\n",
    "print(len(imp_alpha_list))\n",
    "\n",
    "# 1) MAKE ALPHA LIST\n",
    "alpha_list = [utils.generate_alpha(alpha[0], region=REGION, universe=UNIVERSE,neutralization=alpha[1],decay = alpha[2]) for alpha in imp_alpha_list]\n",
    "\n",
    "# 2) MULTISIMULATE ALPHAS\n",
    "iteration = 0\n",
    "result = []\n",
    "for x in range(0,len(alpha_list),10):\n",
    "    iteration += 1\n",
    "    try:\n",
    "        result.append(utils.simulate_alpha_list_multi(s, alpha_list[x:x+10 if x+10 < len(alpha_list) else len(alpha_list)]))\n",
    "        temp_df = pd.concat([\\\n",
    "            pd.concat([pd.DataFrame(pd.concat([pd.Series(data = [res['simulate_data']['regular']],index = ['regular']),pd.Series(res['simulate_data']['settings']),res['is_stats'].iloc[0],pd.Series(data =  res['is_tests'].result.values , index = res['is_tests'].name)])).T for res in  result[-1]]).reset_index(drop=True),\\\n",
    "            pd.DataFrame(data = zip(*[[imp_alpha_list[x][3] for x in range(x,x+10)],[imp_alpha_list[x][4] for x in range(x,x+10)]]),columns = ['mother_alpha','grandmother_alpha']).reset_index(drop=True)]\\\n",
    "            ,axis=1)\n",
    "            \n",
    "        if x == 0:\n",
    "            if os.path.exists(DIR_IMPROVEMENT+f\"/improved_time_horizon.csv\"):\n",
    "                res_df = pd.read_csv(DIR_IMPROVEMENT+f'/improved_time_horizon.csv').drop('Unnamed: 0',axis=1)\n",
    "                res_df = pd.concat([res_df,temp_df])\n",
    "            else:\n",
    "                res_df = temp_df\n",
    "        else:\n",
    "            res_df = pd.concat([res_df,temp_df])\n",
    "            \n",
    "        res_df.reset_index(drop=True).to_csv(DIR_IMPROVEMENT+f'/improved_time_horizon.csv')\n",
    "    except:\n",
    "        print(f'err, iteration = {iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check production correlation\n",
    "check production correlation to verify that the alpha is submittable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all alphas from all single alpha directories\n",
    "total_df = pd.concat(sum([[pd.read_csv(f'./{folder_name}/{file_name}').drop('Unnamed: 0',axis =1) for file_name in os.listdir(f'./{folder_name}/')] for folder_name in ['1_SINGLE_SIGNAL','layer2_2','layer3_2','improved']],[])).reset_index(drop=True)\n",
    "\n",
    "# 1) FILTER ALPHAS WHICH PASSED ALL TESTS\n",
    "all_pass_alphas = total_df.query(\"\"\"\n",
    "LOW_SHARPE == 'PASS' &\\\n",
    "LOW_FITNESS == 'PASS' &\\\n",
    "LOW_TURNOVER == 'PASS' &\\\n",
    "HIGH_TURNOVER == 'PASS' &\\\n",
    "CONCENTRATED_WEIGHT == 'PASS' &\\\n",
    "IS_LADDER_SHARPE == 'PASS' &\\\n",
    "LOW_SUB_UNIVERSE_SHARPE == 'PASS'\n",
    "    \"\"\", engine='python').copy().reset_index(drop=True)\n",
    "\n",
    "# 2) GET INFORMATION FROM API AND SAVE IT INTO MAX_CORREL COLUMN AND HIGH_CORRL_ALPHAS COLUMN \n",
    "all_pass_alphas['max_correl'] = ['not_calculated']*len(all_pass_alphas)\n",
    "all_pass_alphas['high_correl_alphas'] = ['not_calculated']*len(all_pass_alphas)\n",
    "max_correls = []\n",
    "for i in tqdm(range(len(all_pass_alphas))):\n",
    "    try:\n",
    "        max_co = hf.get_max_prod_corr(s,all_pass_alphas.iloc[i]['alpha_id'])\n",
    "        all_pass_alphas.loc[i,'max_correl'] = max_co[0]\n",
    "        all_pass_alphas.loc[i,'high_correl_alphas'] = max_co[1]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "all_pass_alphas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
